
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Feedforward Propagation &#8212; Introduction&lt;br&gt; to Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Backpropagation Algorithm" href="training_2_backprop.html" />
    <link rel="prev" title="Training Neural Networks" href="training_0_intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction<br> to Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction to Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/teaching_philosophy.html">
   Teaching philosophy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/instructor.html">
   Instructor &amp; Credits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/tools.html">
   Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/linear_regression.html">
   Warm-up: Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_1_notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_2_cost_function.html">
     Cost Function in Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_3_gradient_descent_1d.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_4_gradient_descent_multiD.html">
     Multivariate Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_5_learning_rate.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_6_gradient_descent_in_practice.html">
     Gradient Descent in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/logistic_regression.html">
   Logistic Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_1_intro.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_2_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_3_cost_function.html">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_4_gradient_descent.html">
     Gradient Descent for Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_5_multiclass.html">
     Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/model_evaluation.html">
   Model Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_1_train_test_split.html">
     Splitting Datasets for Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_2_perf_metrics.html">
     Performance Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_3_roc_curve.html">
     Let’s ROC!
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_4_bias_variance_tradeoff.html">
     Bias &amp; Variance: a Tradeoff
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision trees and boosting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/dt_0_and_boosting.html">
   Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_1_what_are_trees.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_2_cart_algorithm.html">
     The CART Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_3_limitations.html">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/el_0_intro.html">
   Ensemble Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_1_what_is_ensemble.html">
     What is Ensemble Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_2_random_forests.html">
     Random Forests
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/boost_0_intro.html">
   Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_1_what_is_boosting.html">
     What is Boosting?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_2_adaboost.html">
     AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_3_gradient_boosting.html">
     Gradient Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_0_intro.html">
   Neural Networks: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nn_1_motivations.html">
   Motivations
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="model_rep.html">
   Essential Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_1_neurons.html">
     The basic units: neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_2_activation_functions.html">
     Activation Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_3_loss_cost.html">
     Loss and Cost Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="training_0_intro.html">
   Training Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Feedforward Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_2_backprop.html">
     Backpropagation Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_3_init.html">
     Initialization schemes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optim_0_intro.html">
   Optimizing Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_1_sdg.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_2_hyperparams.html">
     Hyperparameter Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_3_lr_scheduling.html">
     Learning Rate Schedulers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_4_adaptive_methods.html">
     Adaptive Optimizers
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/nn/training_1_forward_prop.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-feedforward-propagation">
   What is Feedforward Propagation?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-by-step-calculations">
   Step by step calculations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-first-hidden-layer">
     Computation of the first hidden layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-second-hidden-layer">
     Computation of the second hidden layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-third-hidden-layer">
     Computation of the third hidden layer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-rule-for-forward-propagation">
   General rule for Forward Propagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Feedforward Propagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-feedforward-propagation">
   What is Feedforward Propagation?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-by-step-calculations">
   Step by step calculations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-first-hidden-layer">
     Computation of the first hidden layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-second-hidden-layer">
     Computation of the second hidden layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-of-the-third-hidden-layer">
     Computation of the third hidden layer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-rule-for-forward-propagation">
   General rule for Forward Propagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="feedforward-propagation">
<h1>Feedforward Propagation<a class="headerlink" href="#feedforward-propagation" title="Permalink to this headline">#</a></h1>
<section id="what-is-feedforward-propagation">
<h2>What is Feedforward Propagation?<a class="headerlink" href="#what-is-feedforward-propagation" title="Permalink to this headline">#</a></h2>
<p>This is one of the key steps in the training of a neural network. It comes after initialization of the network (all weights and biases), which we will covered later. The forward direction means going from the input to the output nodes.</p>
<div class="proof definition admonition" id="feedforwardpropdef">
<p class="admonition-title"><span class="caption-number">Definition 61 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>Feedforward Propagation</strong>, also called <strong>Forward Pass</strong>, is the process consisting of computing and storing all network nodes’ output values, starting with the first hidden layer until the last output layer, using at start either a subset or the entire dataset samples.</p>
</section>
</div><p>Forward propagation thus leads to a list of the neural network predictions for each data instance row used as input. At each node, the computation is the key equation <a class="reference internal" href="model_rep_1_neurons.html#equation-aneq">(69)</a> we saw in the previous sub-section <a class="reference internal" href="model_rep_1_neurons.html#artif-neuron"><span class="std std-ref">Artificial neuron</span></a>, written again for convenience:</p>
<div class="math notranslate nohighlight" id="equation-sumwixieq">
<span class="eqno">(88)<a class="headerlink" href="#equation-sumwixieq" title="Permalink to this equation">#</a></span>\[\hat{y} = f\left(\sum_{j=1}^n w_j x_j + b \right)\]</div>
<p>Let’s define everything in the next subsection.</p>
</section>
<section id="notations">
<h2>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">#</a></h2>
<p>Let’s say we have the following network with <span class="math notranslate nohighlight">\(x_n\)</span> input features, one first hidden layer with <span class="math notranslate nohighlight">\(q\)</span> activation units and a second one with <span class="math notranslate nohighlight">\(r\)</span> activation units. For simplicity, we will choose an output layer with only one node:</p>
<figure class="align-default" id="training-1-nn-notations">
<a class="reference internal image-reference" href="../_images/training_1_nn_notations.png"><img alt="../_images/training_1_nn_notations.png" src="../_images/training_1_nn_notations.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">. A feedforward neural network with the notation we will use for the forward propagation equations (more in text).<br />
<sub>Image from the author</sub></span><a class="headerlink" href="#training-1-nn-notations" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There are lots of subscripts and upperscripts here. Let’s explain the conventions we will use.</p>
<p><strong>Input data</strong><br />
We saw in Lecture 2 that the dataset in supervised learning can be represented as a matrix <span class="math notranslate nohighlight">\(X\)</span> of <span class="math notranslate nohighlight">\(m\)</span> data instances (rows) of <span class="math notranslate nohighlight">\(n\)</span> input features (columns). For clarity in the notations, we will focus for now on only one data instance, the <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> sample. We will note it as a column vector <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{x}^{(i)} = \begin{pmatrix} 
x^{(i)}_1 \\
x^{(i)}_2 \\
\vdots \\
x^{(i)}_n \\
\end{pmatrix}\end{split}\]</div>
<p>The vector elements are all the features in the data. The upperscript indicates the sample index <span class="math notranslate nohighlight">\(i\)</span>, going from 1 to <span class="math notranslate nohighlight">\(m\)</span>, the total number of samples in the dataset.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The layer numbering starts at the first hidden layer where <span class="math notranslate nohighlight">\(\ell=1\)</span>.</p>
</aside>
<p><strong>Activation units</strong><br />
In a given layer <span class="math notranslate nohighlight">\(\ell = 1, 2, \cdots, N^\text{layer}\)</span>, the activation units will give outputs that we will note as a column vector as well:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{a}^{(i, \: \ell)} = \begin{pmatrix} 
 a_1^{(i, \: \ell)}\\
a_2^{(i, \: \ell)},\\
\vdots\\
a_q^{(i, \: \ell)} 
\end{pmatrix}  \;, \end{split}\]</div>
<p>where subscript is the row of the activation unit in the layer, starting from the top. The upperscript indicates the sample index <span class="math notranslate nohighlight">\(i\)</span> and the layer number <span class="math notranslate nohighlight">\(\ell\)</span>. Why the presence of the sample here? We will see soon that these activation units will get a different value for each data sample.</p>
<p><strong>Biases</strong><br />
The biases are also column vectors, one for each layer it connects to and of dimension the number of nodes in that layer:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{b}^{(\ell)} = \begin{pmatrix}
b_1^{(\ell)} \\
b_2^{(\ell)} \\
\vdots \\
b_q^{(\ell)}
\end{pmatrix}\end{split}\]</div>
<p>If the last layer is only made of one node like in our example above, then <span class="math notranslate nohighlight">\(b^{(L)}\)</span> is a scalar. Note that the biases do not depend on the sample index <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Weights</strong><br />
Now the weights. You may see in the literature different ways to represent them. In here we use a convention we could write as:</p>
<div class="math notranslate nohighlight">
\[w^\ell_{j \; \to \; k}\]</div>
<p>In other words, the weights from the layer <span class="math notranslate nohighlight">\(\ell - 1\)</span> to the layer <span class="math notranslate nohighlight">\(\ell\)</span> have as their first index the row of the node from the previous layer (departing node of the weight’s arrow). The second index is the row of the node on layer <span class="math notranslate nohighlight">\(\ell\)</span> to which the weight arrrow points to. The weight <span class="math notranslate nohighlight">\(w^\ell_{j \; \to \; k}\)</span> is the weight departing from node <span class="math notranslate nohighlight">\(j\)</span> on layer <span class="math notranslate nohighlight">\(\ell - 1\)</span> and connecting node <span class="math notranslate nohighlight">\(k\)</span> on layer <span class="math notranslate nohighlight">\(\ell\)</span>. For instance <span class="math notranslate nohighlight">\(w^{(2)}_{3,1}\)</span> is the weight from the third node of layer (1) going to the first node of layer (2).</p>
<p>We can actually represent each weight from layer <span class="math notranslate nohighlight">\(\ell -1\)</span> to layer <span class="math notranslate nohighlight">\(\ell\)</span> as a matrix <span class="math notranslate nohighlight">\(W^{(\ell)}\)</span>. If the previous layer <span class="math notranslate nohighlight">\(\ell -1\)</span> has <span class="math notranslate nohighlight">\(n\)</span> activation units and the layer <span class="math notranslate nohighlight">\(\ell\)</span> has <span class="math notranslate nohighlight">\(q\)</span> activation units, we will have:</p>
<div class="math notranslate nohighlight" id="equation-wmatrixeq">
<span class="eqno">(89)<a class="headerlink" href="#equation-wmatrixeq" title="Permalink to this equation">#</a></span>\[\begin{split}W^{(\ell)} = \begin{pmatrix}
w_{1,1}^{(\ell)} &amp; w_{1,2}^{(\ell)} &amp; \cdots &amp; w_{1,q}^{(\ell)} \\[2ex]
w_{2,1}^{(\ell)} &amp; w_{2,2}^{(\ell)} &amp; \cdots &amp; w_{2,q}^{(\ell)} \\[1ex]
\vdots  &amp; \vdots &amp; \ddots   &amp; \vdots \\[1ex]
w_{n,1}^{(\ell)} &amp; w_{n,2}^{(\ell)} &amp;  \cdots &amp; w_{n,q}^{(\ell)} \\
\end{pmatrix}\end{split}\]</div>
<p>Note that we do not have an index <span class="math notranslate nohighlight">\(i\)</span> for the weight matrix <span class="math notranslate nohighlight">\(W^{(\ell)}\)</span>. Why? Because the weights are unique for a given network. In fact the weights – and the biases – are optimized after the network has incorporated all the data samples. We will actually determine the optimal weights and biases in the next chapter after.</p>
<p>Let’s now see how we calculate all the values of the activation units!</p>
</section>
<section id="step-by-step-calculations">
<h2>Step by step calculations<a class="headerlink" href="#step-by-step-calculations" title="Permalink to this headline">#</a></h2>
<section id="computation-of-the-first-hidden-layer">
<h3>Computation of the first hidden layer<a class="headerlink" href="#computation-of-the-first-hidden-layer" title="Permalink to this headline">#</a></h3>
<p>Let’s use Equation <a class="reference internal" href="#equation-sumwixieq">(88)</a> to compute the activation unit outputs of the first layer. The activation function is represented as <span class="math notranslate nohighlight">\(f\)</span> here. So for a given data sample <span class="math notranslate nohighlight">\(i\)</span>, we have:</p>
<div class="math notranslate nohighlight" id="equation-firstlayereq">
<span class="eqno">(90)<a class="headerlink" href="#equation-firstlayereq" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align*}
a^{(i, \: 1)}_1 &amp;= f\left(\; w_{1,1}^{(1)} \; x^{(i)}_1 \;+\; w_{2,1}^{(1)} \; x^{(i)}_2 \;+\; \cdots + \; w_{n,1}^{(1)} \; x^{(i)}_n \;+\; b^{(1)}_1\right)\\[2ex]
a^{(i, \: 1)}_2 &amp;= f\left(\; w_{1,2}^{(1)} \; x^{(i)}_1 \;+\; w_{2,2}^{(1)} \; x^{(i)}_2 \;+\; \cdots + \; w_{n,2}^{(1)} \; x^{(i)}_n \;+\; b^{(1)}_2\right)\\
&amp;\vdots \\[2ex]
a^{(i, \: 1)}_q &amp;= f\left(\; w_{1,q}^{(1)} \; x^{(i)}_1 \;+\; w_{2,q}^{(1)} \; x^{(i)}_2 \;+\; \cdots + \; w_{n,q}^{(1)} \; x^{(i)}_n \;+\; b^{(1)}_q\right)\\
\end{align*}\end{split}\]</div>
<p>We can actually write it in the matrix form. Let’s first write it in an expanded version with the matrix elements:</p>
<div class="math notranslate nohighlight" id="equation-firstlayermatrixexpandedeq">
<span class="eqno">(91)<a class="headerlink" href="#equation-firstlayermatrixexpandedeq" title="Permalink to this equation">#</a></span>\[\begin{split}\boldsymbol{a}^{(i, \: 1)} = f\left[ \; 
\begin{pmatrix}w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} &amp; \cdots &amp; w_{1,q}^{(1)} \\[2ex]w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} &amp; \cdots &amp; w_{2,q}^{(1)} \\[1ex]\vdots  &amp; \vdots &amp; \ddots   &amp; \vdots \\[1ex]w_{n,1}^{(1)} &amp; w_{n,2}^{(1)} &amp;  \cdots &amp; w_{n,q}^{(1)} \\\end{pmatrix}^\top
\begin{pmatrix} 
x^{(i)}_1 \\
x^{(i)}_2 \\
\cdots \\
x^{(i)}_n \\
\end{pmatrix} 
 \;+\; \begin{pmatrix}
b_1^{(1)} \\
b_2^{(1)} \\
\cdots \\
b_q^{(1)}
\end{pmatrix} \; \right]\end{split}\]</div>
<p>You can verify that <span class="math notranslate nohighlight">\(\boldsymbol{a}^{(i, \: 1)}\)</span> will be a column vector with <span class="math notranslate nohighlight">\(q\)</span> elements.
This can be written in a compact way:</p>
<div class="math notranslate nohighlight" id="equation-firstlayermatrixeq">
<span class="eqno">(92)<a class="headerlink" href="#equation-firstlayermatrixeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{a}^{(i, \: 1)} = f\left[ \; \left(W^{(1)}\right)^\top \; \boldsymbol{x}^{(i)} \;+\; \boldsymbol{b}^{(1)} \;\right]\]</div>
<p>Much lighter.</p>
</section>
<section id="computation-of-the-second-hidden-layer">
<h3>Computation of the second hidden layer<a class="headerlink" href="#computation-of-the-second-hidden-layer" title="Permalink to this headline">#</a></h3>
<p>Let’s do the same calculation for the second layer of activation units. Instead of the dataset vector <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span>, we will have <span class="math notranslate nohighlight">\(\boldsymbol{a}^{(i, \: 1)}\)</span> as input:</p>
<div class="math notranslate nohighlight" id="equation-secondlayermatrixexpandedeq">
<span class="eqno">(93)<a class="headerlink" href="#equation-secondlayermatrixexpandedeq" title="Permalink to this equation">#</a></span>\[\begin{split}\boldsymbol{a}^{(i, \: 2)} = f\left[ \; 
\begin{pmatrix}w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} &amp; \cdots &amp; w_{1,r}^{(2)} \\[2ex]w_{2,1}^{(2)} &amp; w_{2,2}^{(2)} &amp; \cdots &amp; w_{2,r}^{(2)} \\[1ex]\vdots  &amp; \vdots &amp; \ddots   &amp; \vdots \\[1ex]w_{q,1}^{(2)} &amp; w_{q,2}^{(2)} &amp;  \cdots &amp; w_{q,r}^{(2)} \\\end{pmatrix}^\top
\begin{pmatrix} 
a^{(i, \: 1)}_1 \\
a^{(i, \: 1)}_2 \\
\cdots \\
a^{(i, \: 1)}_q \\
\end{pmatrix} 
 \;+\; \begin{pmatrix}
b_1^{(2)} \\
b_2^{(2)} \\
\cdots \\
b_r^{(2)}
\end{pmatrix} \; \right]\end{split}\]</div>
<p>And the elegant, light version:</p>
<div class="math notranslate nohighlight" id="equation-secondlayermatrixeq">
<span class="eqno">(94)<a class="headerlink" href="#equation-secondlayermatrixeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{a}^{(i, \: 2)} = f\left[ \; \left(W^{(2)}\right)^\top \; \boldsymbol{a}^{(i, \: 1)} \;+\; \boldsymbol{b}^{(2)} \;\right]\]</div>
<p>We start seeing a pattern here looking at the matricial equations <a class="reference internal" href="#equation-firstlayermatrixeq">(92)</a> and <a class="reference internal" href="#equation-secondlayermatrixeq">(94)</a>. More on this soon in Section <a class="reference internal" href="#nn1-forwardprop-rule"><span class="std std-ref">General rule for Forward Propagation</span></a>. Let’s finish the process with the last layer.</p>
</section>
<section id="computation-of-the-third-hidden-layer">
<h3>Computation of the third hidden layer<a class="headerlink" href="#computation-of-the-third-hidden-layer" title="Permalink to this headline">#</a></h3>
<p>With one output node, it is actually simpler than for the hidden layers above. We can still write it in the same form as Equation <a class="reference internal" href="#equation-secondlayermatrixeq">(94)</a>:</p>
<div class="math notranslate nohighlight" id="equation-thirdlayermatrixeq">
<span class="eqno">(95)<a class="headerlink" href="#equation-thirdlayermatrixeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{a}^{(i, \: 3)} = f\left[ \; \left(W^{(3)}\right)^\top \; \boldsymbol{a}^{(i, \: 2)} \;+\; \boldsymbol{b}^{(3)} \;\right]\]</div>
<p>using <span class="math notranslate nohighlight">\(\boldsymbol{a}^{(i, \: 2)}\)</span> that we calculated above. In our case <span class="math notranslate nohighlight">\(\boldsymbol{a}^{(i, \: 3)}\)</span> has only one element: <span class="math notranslate nohighlight">\(a^{(i, \: 3)}_1 = \hat{y}^{(i)}\)</span>. Thus the matrix <span class="math notranslate nohighlight">\(W^{(3)}\)</span> has only one column. The bias ‘vector’ is actually a scalar: <span class="math notranslate nohighlight">\(b^{(3)}\)</span>.</p>
<p>We have computed a value for each activation unit for a given data sample <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span>. That is the end of the forward propagation process! As you can see, it contains lots of calculations. And now you may understand why activation functions that are simple and fast to compute are preferrable, as they intervene each time we compute the output of an activation unit.</p>
<p>Let’s now get a general formula.</p>
</section>
</section>
<section id="general-rule-for-forward-propagation">
<span id="nn1-forwardprop-rule"></span><h2>General rule for Forward Propagation<a class="headerlink" href="#general-rule-for-forward-propagation" title="Permalink to this headline">#</a></h2>
<p>If we rewrite the first layer of inputs for a given sample <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span> as a “layer zero” <span class="math notranslate nohighlight">\(\boldsymbol{a}^{(i, \: 0)}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-xisazeroeq">
<span class="eqno">(96)<a class="headerlink" href="#equation-xisazeroeq" title="Permalink to this equation">#</a></span>\[\begin{split}\boldsymbol{x}^{(i)} = \begin{pmatrix} 
x^{(i, \: 0)}_1 \\
x^{(i, \: 0)}_2 \\
\cdots \\
x^{(i, \: 0)}_n \\
\end{pmatrix} = \begin{pmatrix} 
a^{(i, \: 0)}_1 \\
a^{(i, \: 0)}_2 \\
\cdots \\
a^{(i, \: 0)}_n \\
\end{pmatrix} = \boldsymbol{a}^{(i, \: 0)}\;,\end{split}\]</div>
<p>then we can write a general rule for computing the outputs of a fully connected layer <span class="math notranslate nohighlight">\(\ell\)</span> knowing the outputs of the previous layer <span class="math notranslate nohighlight">\(\ell -1\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-genrulefeedforwardeq">
<span class="eqno">(97)<a class="headerlink" href="#equation-genrulefeedforwardeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{a}^{(i, \: \ell)} = f\left[ \; \left(W^{(\ell)}\right)^\top \; \boldsymbol{a}^{(i, \: \ell -1)} \;+\; \boldsymbol{b}^{(\ell)} \;\right]\]</div>
<p>This is the general rule for computing all outputs of a fully connected feedforward neural network.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h2>
<p>Feedforward propagation is the computation of the values of all activation units of a fully connected feedforward neural network.</p>
<p>As the process includes the last layer (output), feedforward propagation also leads to predictions.</p>
<p>These predictions will be compared to the observed values.</p>
<p>Feedforward propagation is a step in the training of a neural network.</p>
<p>The next step of the training is to go ‘backward’, from the output error <span class="math notranslate nohighlight">\(\hat{y}^\text{pred} - y^\text{obs}\)</span> to the first layer to then adjust all weights using a gradient-based procedure. This is the core of backpropagation, which we will cover in the next section.</p>
<div class="seealso admonition">
<p class="admonition-title">Learn More</p>
<p>Very nice animations <a class="reference external" href="https://yogayu.github.io/DeepLearningCourse/03/ForwardPropagation.html">here</a> illustrating the forward propagation process.<br />
Source: <a class="reference external" href="https://yogayu.github.io/DeepLearningCourse/intro.html">Xinyu You’s course <em>An online deep learning course for humanists</em></a></p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nn"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="training_0_intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Training Neural Networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="training_2_backprop.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Backpropagation Algorithm</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    <div class="extra_footer">
      <div style="display:flex; align-items:center; gap:10px;">
  <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    <img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="CC Logo">
  </a>
  <div>
    By Claire David • © 2025<br>
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
  </div>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>