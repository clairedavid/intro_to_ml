
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Adaptive Optimizers &#8212; Introduction&lt;br&gt; to Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Train Your Neural Net!" href="train_your_nn.html" />
    <link rel="prev" title="Learning Rate Schedulers" href="optim_3_lr_scheduling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction<br> to Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction to Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/teaching_philosophy.html">
   Teaching philosophy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/instructor.html">
   Instructor &amp; Credits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/tools.html">
   Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/linear_regression.html">
   Warm-up: Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_1_notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_2_cost_function.html">
     Cost Function in Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_3_gradient_descent_1d.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_4_gradient_descent_multiD.html">
     Multivariate Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_5_learning_rate.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/lin_reg_6_gradient_descent_in_practice.html">
     Gradient Descent in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/logistic_regression.html">
   Logistic Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_1_intro.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_2_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_3_cost_function.html">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_4_gradient_descent.html">
     Gradient Descent for Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/log_reg_5_multiclass.html">
     Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basics/model_evaluation.html">
   Model Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_1_train_test_split.html">
     Splitting Datasets for Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_2_perf_metrics.html">
     Performance Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_3_roc_curve.html">
     Let’s ROC!
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basics/model_eval_4_bias_variance_tradeoff.html">
     Bias &amp; Variance: a Tradeoff
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision trees and boosting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/dt_0_and_boosting.html">
   Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_1_what_are_trees.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_2_cart_algorithm.html">
     The CART Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_3_limitations.html">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/el_0_intro.html">
   Ensemble Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_1_what_is_ensemble.html">
     What is Ensemble Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_2_random_forests.html">
     Random Forests
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/boost_0_intro.html">
   Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_1_what_is_boosting.html">
     What is Boosting?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_2_adaboost.html">
     AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_3_gradient_boosting.html">
     Gradient Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_0_intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nn_1_motivations.html">
   Motivations
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="model_rep.html">
   Essential Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_1_neurons.html">
     The basic units: neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_2_activation_functions.html">
     Activation Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_rep_3_loss_cost.html">
     Loss and Cost Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="training_0_intro.html">
   Training Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="training_1_forward_prop.html">
     Feedforward Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_2_backprop.html">
     Backpropagation Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_3_init.html">
     Initialization Schemes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="optim_0_intro.html">
   Optimizing Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="optim_1_sdg.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_2_hyperparams.html">
     Hyperparameter Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optim_3_lr_scheduling.html">
     Learning Rate Schedulers
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Adaptive Optimizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="train_your_nn.html">
   Train Your Neural Net!
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_0_intro.html">
   Learning Without Labels?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_1_kmeans.html">
   Clustering: k-Means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_2_pca.html">
   Principal Component Analysis (PCA)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_3_autoencoders.html">
   Autoencoders
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  STEP UP!
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../studies/studies_what_is_it.html">
   What is STEP?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../studies/studies_list.html">
     List of STEPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../studies/studies_submit_yours.html">
     Submit Your Own STEP!
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  AI Ethics &amp; Outlook
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ethics_outlook/ethics_0_intro.html">
   AI Ethics: What is it?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_1_definitions.html">
     Definitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_2_why_matters.html">
     Why Does It Matter?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_3_resources.html">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ethics_outlook/outlook.html">
   Outlook
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorial area
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t00_setup.html">
   Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t01_linear_regressor.html">
   T1. Linear Regressor
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t02_classifier.html">
   T2. Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t03_decision_stump.html">
   T3. Decision Stump
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t04_forestry.html">
   T4. Forestry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t05_nn_by_hand.html">
   T5. Neural Network by Hand!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t06_unsupervised.html">
   T6. Unsupervised algorithms
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/nn/optim_4_adaptive_methods.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adagrad">
   AdaGrad
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rmsprop">
   RMSProp
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#momentum-optimization">
   Momentum Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adam">
   Adam
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Adaptive Optimizers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adagrad">
   AdaGrad
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rmsprop">
   RMSProp
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#momentum-optimization">
   Momentum Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adam">
   Adam
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="adaptive-optimizers">
<h1>Adaptive Optimizers<a class="headerlink" href="#adaptive-optimizers" title="Permalink to this headline">#</a></h1>
<p>Before the arrival of faster methods, combining Stochastic Gradient Descent with a learning rate schedule was considered close to state-of-the-art. Adaptive optimizers changed this by embedding the learning rate adjustment directly into the optimization process. Instead of relying on a separate scheduler, they use feedback from the model to adjust updates based on past gradients. The key advantage: they require little to no manual tuning.</p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 75 </span></p>
<section class="definition-content" id="proof-content">
<p>An <strong>Adaptative Learning Rate</strong> is a technique that varies the learning rate using feedback from the model itself.</p>
</section>
</div><p>As the variation of the learning rate is done by the optimizer, the terms adaptive learning rate and adaptive optimizer are often used interchangeably.</p>
<p>Below are brief descriptions of the most popular adaptative optimizers.</p>
<section id="adagrad">
<h2>AdaGrad<a class="headerlink" href="#adagrad" title="Permalink to this headline">#</a></h2>
<p>This first adaptative algorithm was published in 2011. Compared to the classical Gradient Descent, AdaGrad points more directly toward the global optimum by decaying the learning rate more on the steepest dimension.</p>
<figure class="align-default" id="optim-4-adagrad">
<a class="reference internal image-reference" href="../_images/optim_4_adagrad.png"><img alt="../_images/optim_4_adagrad.png" src="../_images/optim_4_adagrad.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text">. The Gradient Descent (blue) vs AdaGrad (orange).<br />
<sub>Image: Aurélien Géron, <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Second Edition</em></sub></span><a class="headerlink" href="#optim-4-adagrad" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In the figure above, the AdaGrad takes a more direct thus shorter path than Gradient Descent, as AdaGrad has its learning rate reduced in the direction of steepest descent. This is done by squaring each gradient component into a vector <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>. The steepest a gradient component, the larger the square of this component <span class="math notranslate nohighlight">\(s_j\)</span>. The weights are updated in almost the same way as with Gradient Descent, yet each component is divided by <span class="math notranslate nohighlight">\(\boldsymbol{s} + \epsilon\)</span>. The added term <span class="math notranslate nohighlight">\(\epsilon\)</span> is to prevent a division by zero (typically 10<span class="math notranslate nohighlight">\(^{-10}\)</span>). As a result, the algorithm detects how to change the learning rate dynamically and specifically on the large gradient components to adapt to their steep slope.</p>
<p>Mathematically:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The <span class="math notranslate nohighlight">\(\otimes\)</span> symbol is the element-wise multiplication, while the <span class="math notranslate nohighlight">\(\oslash\)</span> is the element-wise division.</p>
</aside>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\boldsymbol{s} \; &amp;\leftarrow \; \boldsymbol{s}  \;+\;  \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \;\otimes \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \\[2ex]
\boldsymbol{W} \; &amp;\leftarrow \; \boldsymbol{W} - \alpha \; \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \oslash \sqrt{\boldsymbol{s} + \epsilon}
\end{align*}\end{split}\]</div>
<p><strong>The con</strong><br />
AdaGrad performs well on simple problem, like linear regression. With neural networks, it tends to stop too soon, before reaching the global minimum. Luckily, other adaptative algorithms fix this.</p>
</section>
<section id="rmsprop">
<h2>RMSProp<a class="headerlink" href="#rmsprop" title="Permalink to this headline">#</a></h2>
<p>RMSProp stands for Root Mean Square Propagation. It works the same as AdaGrad except that it keeps track of an exponentially decaying average of past squared gradients:</p>
<div class="math notranslate nohighlight" id="equation-rmspropeq">
<span class="eqno">(139)<a class="headerlink" href="#equation-rmspropeq" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align*}
\boldsymbol{s} \; &amp;\leftarrow \; \beta\boldsymbol{s}  \;+\;  (1 - \beta) \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \;\otimes \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \\[2ex]
\boldsymbol{W} \; &amp;\leftarrow \; \boldsymbol{W} - \alpha \; \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \oslash \sqrt{\boldsymbol{s} + \epsilon}
\end{align*}\end{split}\]</div>
<p>with the decay rate <span class="math notranslate nohighlight">\(\beta\)</span>, between 0 and 1, yet typically set to 0.9. One might ask: what is the point to introduce an extra hyparameter? It turns out the default value works well in most cases and does not require tuning. What the update in the expressions <a class="reference internal" href="#equation-rmspropeq">(139)</a> shows is a recursive way of computing a so-called Exponential Moving Average (EMA). In other words, <span class="math notranslate nohighlight">\(\beta\)</span> acts as a constant smoothing factor, representing the degree of weighting increase. A lower <span class="math notranslate nohighlight">\(\beta\)</span> discounts older observations faster. A higher <span class="math notranslate nohighlight">\(\beta\)</span> gives more weight to the previous gradient, a bit less weight to the previous previous gradient, etc.</p>
<p>For more elaborated tasks than the simple linear case, RMSProp is robust. It reigned until dethrowned by a newcomer called Adam. Before introducing Adam, let’s first cover the notion of momentum optimization.</p>
</section>
<section id="momentum-optimization">
<h2>Momentum Optimization<a class="headerlink" href="#momentum-optimization" title="Permalink to this headline">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The algorithm name borrows the momentum concept from physics, yet it is a only metaphor.</p>
</div>
</aside>
<p>In physics, the momentum <span class="math notranslate nohighlight">\(\boldsymbol{p}\)</span> is a vector obtained by taking the product of the mass and velocity of an object. It quantifies motion. In computing science, momentum refers to the direction and speed at which the parameters move - via iterative updates - through the parameter space. With momentum optimization, inertia is added to the system by updating the weights using the momenta from past iterations. This keeps the update in the same direction. The common analogy is a ball rolling down on a curved surface. It will start slowly but soon “gain momentum”, thus can go through flat gradient surface much faster than with the classic Gradient Descent. Adding momentum considerably speeds the Gradient Descent process. It also helps roll beside local minimum.</p>
<p>The momentum optimization algorithm is as follow:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\boldsymbol{m} \; &amp;\leftarrow \; \beta \boldsymbol{m} \; - \; \alpha \; \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \\[2ex]
\boldsymbol{W} \; &amp;\leftarrow \; \boldsymbol{W} + \boldsymbol{m} 
\end{align*}\end{split}\]</div>
<p>Here the <span class="math notranslate nohighlight">\(\beta\)</span> parameter controls the momentum from becoming too large. It could be analogous to introducing friction; <span class="math notranslate nohighlight">\(\beta\)</span> ranges from 0 to 1, with 0 meaning high friction and 1 no friction at all. In the literature you will see a common default value of <span class="math notranslate nohighlight">\(\beta\)</span> = 0.9.</p>
</section>
<section id="adam">
<h2>Adam<a class="headerlink" href="#adam" title="Permalink to this headline">#</a></h2>
<p>Adam stands for <em>Adaptative moment estimation</em>. It merges RMSProp with momentum optimization. Recall that RMSProp uses an exponentially decaying average of past squared gradients, while momentum does the same except with gradients (not squared).</p>
<p>Mathematically:</p>
<div class="math notranslate nohighlight" id="equation-adameq">
<span class="eqno">(140)<a class="headerlink" href="#equation-adameq" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align*}
\boldsymbol{m} \; &amp;\leftarrow \; \beta_1 \boldsymbol{m} \; - \; (1 - \beta_1) \; \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}}\\[1ex]
\boldsymbol{s} \; &amp;\leftarrow \; \; \beta_2\boldsymbol{s}  \;+\;  (1 - \beta_2) \; \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \otimes  \frac{\partial J\left(\boldsymbol{W}\right)}{\partial \boldsymbol{W}} \\[1ex]
\boldsymbol{\widehat{m}} &amp;\leftarrow \frac{\boldsymbol{m}}{1 - \beta_1^t} \\[1ex]
\boldsymbol{\widehat{s}} &amp;\leftarrow \frac{\boldsymbol{s}}{1 - \beta_2^t} \\[1ex]
\boldsymbol{W} \; &amp;\leftarrow \; \boldsymbol{W} + \alpha \; \boldsymbol{\widehat{m}} \oslash \sqrt{\boldsymbol{\widehat{s}} + \epsilon}
\end{align*} \end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(t\)</span> the iteration number.</p>
<p>The first step is not exactly the momentum; instead of an exponentially decaying sum, Adam computes an exponentially decaying average.</p>
<p>The algorithm seems complicated at first glance, especially steps including the averages of <span class="math notranslate nohighlight">\(\boldsymbol{\widehat{m}}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\widehat{s}}\)</span>. The division with the betas is to boost at the start of the training the values or <span class="math notranslate nohighlight">\(\boldsymbol{m}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, which are initialized at zero, in order to not stay at zero.</p>
<p>“Many extra parameters on top of the learning rate, so what is the point?” will you say. Actually the momentum decay <span class="math notranslate nohighlight">\(\beta_1\)</span> is set to 0.9, the scaling one <span class="math notranslate nohighlight">\(\beta_2\)</span> to 0.999 and <span class="math notranslate nohighlight">\(\epsilon\)</span> to 10<span class="math notranslate nohighlight">\(^{-7}\)</span>. And if the learning rate is still a hyperparameter (usually set at 0.001 at start), it will be adapted to the optimiziation problem at hand by the algorithm. In that sense, the Adam optimizer is almost parameter free.</p>
<div class="seealso admonition">
<p class="admonition-title">Learn More</p>
<ul class="simple">
<li><p>Article: <a class="reference external" href="https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/">How to Configure the Learning Rate When Training Deep Learning Neural Networks</a> (machinelearningmastery.com)</p></li>
<li><p>Article: <a class="reference external" href="https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/">Gradient Descent With Momentum from Scratch</a> (machinelearningmastery.com)</p></li>
<li><p>Paper: <a class="reference external" href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> (JMLR)</p></li>
<li><p>There is no paper for RMSProp as it was never officially published! One of the authors Geoffrey Hinton presented it in a Coursera lecture. As a consequence, it is amusingly referenced by researchers as: <a class="reference external" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Slide 29 in Lecture 6</a> (Hinton, Coursera, 2012)</p></li>
<li><p>Article: <a class="reference external" href="https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f">Gradient Descent with Momentum</a> (towardsdatascience.com)<br />
👉 helps understand the Exponential Moving Average (EMA).</p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization (2015)</a> (arXiv)</p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nn"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="optim_3_lr_scheduling.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Learning Rate Schedulers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="train_your_nn.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Train Your Neural Net!</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    <div class="extra_footer">
      <div style="display:flex; align-items:center; gap:10px;">
  <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    <img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="CC Logo">
  </a>
  <div>
    By Claire David • © 2025<br>
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
  </div>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>