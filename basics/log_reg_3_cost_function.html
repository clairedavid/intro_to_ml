
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Cost Function for Classification &#8212; Introduction&lt;br&gt; to Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gradient Descent for Logistic Regression" href="log_reg_4_gradient_descent.html" />
    <link rel="prev" title="What is the Sigmoid Function?" href="log_reg_2_sigmoid.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction<br> to Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction to Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/teaching_philosophy.html">
   Teaching philosophy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/instructor.html">
   Instructor &amp; Credits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/tools.html">
   Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The basics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linear_regression.html">
   Warm-up: Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_1_notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_2_cost_function.html">
     Cost Function in Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_3_gradient_descent_1d.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_4_gradient_descent_multiD.html">
     Multivariate Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_5_learning_rate.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_6_gradient_descent_in_practice.html">
     Gradient Descent in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_1_intro.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_2_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_4_gradient_descent.html">
     Gradient Descent for Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_5_multiclass.html">
     Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="model_evaluation.html">
   Model Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_1_train_test_split.html">
     Splitting Datasets for Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_2_perf_metrics.html">
     Performance Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_3_roc_curve.html">
     Let‚Äôs ROC!
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_4_bias_variance_tradeoff.html">
     Bias &amp; Variance: a Tradeoff
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision trees and boosting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/dt_0_and_boosting.html">
   Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_1_what_are_trees.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_2_cart_algorithm.html">
     The CART Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_3_limitations.html">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/el_0_intro.html">
   Ensemble Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_1_what_is_ensemble.html">
     What is Ensemble Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_2_random_forests.html">
     Random Forests
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/boost_0_intro.html">
   Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_1_what_is_boosting.html">
     What is Boosting?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_2_adaboost.html">
     AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_3_gradient_boosting.html">
     Gradient Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/basics/log_reg_3_cost_function.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wavy-least-squares">
   Wavy least squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-new-cost-function">
   Building a new cost function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-for-k-classes">
   Generalization for
   <span class="math notranslate nohighlight">
    \(K\)
   </span>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notations">
     Notations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-softmax">
     The softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-cross-entropy">
     Categorical cross-entropy
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Cost Function for Classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#wavy-least-squares">
   Wavy least squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-new-cost-function">
   Building a new cost function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-for-k-classes">
   Generalization for
   <span class="math notranslate nohighlight">
    \(K\)
   </span>
   classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notations">
     Notations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-softmax">
     The softmax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-cross-entropy">
     Categorical cross-entropy
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="cost-function-for-classification">
<h1>Cost Function for Classification<a class="headerlink" href="#cost-function-for-classification" title="Permalink to this headline">#</a></h1>
<section id="wavy-least-squares">
<h2>Wavy least squares<a class="headerlink" href="#wavy-least-squares" title="Permalink to this headline">#</a></h2>
<p>If we plug our sigmoid hypothesis function <span class="math notranslate nohighlight">\(h_\boldsymbol{\theta}(x)\)</span> into the cost function defined for linear regression (Equation <a class="reference internal" href="lin_reg_2_cost_function.html#equation-costfunctionlinreg">(3)</a> from Lecture <a class="reference internal" href="lin_reg_2_cost_function.html#linreg-cost"><span class="std std-ref">Cost Function for Linear Regression</span></a>), we will have a complex non-linear function that could be non-convex.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Refresher on convex functions</p>
<p>A convex function has at least one global minimum, and if it is strictly convex, the minimum is unique.</p>
<p><strong>Mathematically:</strong><br />
A function <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is <strong>convex</strong> if, for all <span class="math notranslate nohighlight">\(x_1, x_2 \in \mathbb{R}^n\)</span> and for all <span class="math notranslate nohighlight">\(\lambda \in [0,1]\)</span>, we have:</p>
<div class="math notranslate nohighlight" id="equation-convexeq">
<span class="eqno">(18)<a class="headerlink" href="#equation-convexeq" title="Permalink to this equation">#</a></span>\[f\big(\lambda x_1 + (1-\lambda) x_2\big) \leq \lambda f(x_1) + (1-\lambda) f(x_2).\]</div>
<p>Intuitively: a straight line between any two points on the graph of the function lies above the graph itself.</p>
<p>In other words, the function ‚Äúcurves upwards‚Äù or is ‚Äúbowl-shaped.‚Äù</p>
</div>
<p>Consider this function:</p>
<figure class="align-default" id="poly3minima-example" style="width: 80%">
<div class="cell_output docutils container">
<img alt="../_images/log_reg_plots_10_0.png" src="../_images/log_reg_plots_10_0.png" />
</div>
</figure>
<p>Imagine running a gradient descent procedue that starts from a randomly initialized <span class="math notranslate nohighlight">\(\theta_0\)</span> parameter around zero (or worse, lower than -2). It will fall into a local minima. Our cost function will not be at the global minimum! It is crucial to work with a cost function accepting one unique minimum.</p>
</section>
<section id="building-a-new-cost-function">
<h2>Building a new cost function<a class="headerlink" href="#building-a-new-cost-function" title="Permalink to this headline">#</a></h2>
<p>As we saw in the previous section, the sigmoid fits the 1D data distribution very well. Our cost function will use the hypothesis <span class="math notranslate nohighlight">\(h_\boldsymbol{\theta}(\boldsymbol{x})\)</span> function as input. Recall that the hypothesis <span class="math notranslate nohighlight">\(h_\boldsymbol{\theta}(\boldsymbol{x})\)</span> is bounded between 0 and 1. What we need is a cost function producing high values if we mis-classify events and values close to zero if we correctly label the data. Let‚Äôs examine what we want for the two cases:</p>
<p><strong>Case of a signal event:</strong><br />
A data point labelled signal verifies by our convention <span class="math notranslate nohighlight">\(y=1\)</span>. If our hypothesis <span class="math notranslate nohighlight">\(y^\text{pred} = h_\boldsymbol{\theta}(\boldsymbol{x})\)</span> is also 1, then we have a good prediction. The cost value should be zero. If however our signal sample has a wrong prediction <span class="math notranslate nohighlight">\(y^\text{pred} = h_\boldsymbol{\theta}(\boldsymbol{x}) = 0\)</span>, then the cost function should take large values to penalize this bad prediction. We need thus a strictly decreasing function, starting with high values and cancelling at the coordinate (1, 0).</p>
<p><strong>Case of a background event:</strong><br />
The sigmoid can be interpreted as a probability for a sample being signal or not (but note it is not a probability distribution function). As we have only two outcomes, the probability for a data point to be non-signal will be in the form of <span class="math notranslate nohighlight">\(1 - h_\boldsymbol{\theta}(\boldsymbol{x})\)</span>. We want to find a function with this time a zero cost if the prediction <span class="math notranslate nohighlight">\(y^\text{pred} = h_\boldsymbol{\theta}(\boldsymbol{x}) = 0\)</span> and a high cost for an erroneous prediction <span class="math notranslate nohighlight">\(y^\text{pred} = h_\boldsymbol{\theta}(\boldsymbol{x}) = 1\)</span>.</p>
<p>Now let‚Äôs have a look at these two functions:</p>
<figure class="align-default" id="log-h-x" style="width: 100%">
<div class="cell_output docutils container">
<img alt="../_images/log_reg_plots_12_1.png" src="../_images/log_reg_plots_12_1.png" />
</div>
</figure>
<p>For each case, the cost function has only one minimum and harshly penalizes wrong prediction by blowing up at infinity.<br />
How to combine these two into one cost function for logistic regression?<br />
Like this:</p>
<div class="proof definition admonition" id="costFLogRegDef">
<p class="admonition-title"><span class="caption-number">Definition 18 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>cost function for logistic regression</strong> is defined as:</p>
<div class="math notranslate nohighlight" id="equation-costfunctionlogreg">
<span class="eqno">(19)<a class="headerlink" href="#equation-costfunctionlogreg" title="Permalink to this equation">#</a></span>\[C(\boldsymbol{\theta}) = - \frac{1}{m} \sum^m_{i=1} \left[ \;\; {\color{RoyalBlue}y^{(i)} \log\left( h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)} ) \right) }\;\;+\;\; {\color{OliveGreen}(1- y^{(i)}) \log\left( 1 - h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)} ) \right)} \;\;\right]\]</div>
<p>This function is also called <strong>cross-entropy loss function</strong> and is the standard cost function for binary classifiers.</p>
</section>
</div><p>Note the negative sign factorized at the beginning of the equation. The first and second term inside the sum are multiplied by <span class="math notranslate nohighlight">\({\color{RoyalBlue}y^{(i)}}\)</span> and <span class="math notranslate nohighlight">\({\color{OliveGreen}(1 - y^{(i)})}\)</span>. This acts as a ‚Äúswitch‚Äù between the two possible cases for the targets: <span class="math notranslate nohighlight">\({\color{RoyalBlue}y=1}\)</span> and <span class="math notranslate nohighlight">\({\color{OliveGreen}y=0}\)</span>. If <span class="math notranslate nohighlight">\({\color{RoyalBlue}y=1}\)</span>, the second term cancels out and the cost takes the value of the first. If <span class="math notranslate nohighlight">\({\color{OliveGreen}y=0}\)</span>, the first term vanishes. The two mutually exclusive cases are combined into one mathematical expression.</p>
</section>
<section id="generalization-for-k-classes">
<h2>Generalization for <span class="math notranslate nohighlight">\(K\)</span> classes<a class="headerlink" href="#generalization-for-k-classes" title="Permalink to this headline">#</a></h2>
<p>The cross-entropy cost function we defined for binary classification can be naturally extended to handle <span class="math notranslate nohighlight">\(K\)</span> classes.</p>
<section id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">#</a></h3>
<p>Recall the notations we use in binary classification. From an input vector <span class="math notranslate nohighlight">\(\boldsymbol{x} = (x_1, \cdots , x_n)\)</span> of <span class="math notranslate nohighlight">\(n\)</span> input features, we combine it with <span class="math notranslate nohighlight">\(n+1\)</span> model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = (\theta_0, \theta_1, \cdots, \theta_n)\)</span> linearly:</p>
<div class="math notranslate nohighlight" id="equation-sumfeaturerecalleq">
<span class="eqno">(20)<a class="headerlink" href="#equation-sumfeaturerecalleq" title="Permalink to this equation">#</a></span>\[z^{(i)} = \sum_{j=1}^{n_\text{features}} \theta_{j} x_j^{(i)}  = \boldsymbol{\theta}^\top \boldsymbol{x}^{(i)}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>In the following, we will write <span class="math notranslate nohighlight">\(\hat{y}\)</span> instead of <span class="math notranslate nohighlight">\(y^\text{pred}\)</span> to lighten the equations. Both refers to a prediction.</p>
</aside>
<p>So the pipeline for binary classification is:</p>
<div class="math notranslate nohighlight" id="equation-pipelinebinregeq">
<span class="eqno">(21)<a class="headerlink" href="#equation-pipelinebinregeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{x}^{(i)} \xrightarrow{ \text{linear map }\;\boldsymbol{\theta}^\top \boldsymbol{x}^{(i)}\;} z^{(i)}
\xrightarrow{\;\text{logistic function }\sigma(z^{(i)})\;} \hat{y}^{(i)}\]</div>
<p>At the end, the target for a given sample <span class="math notranslate nohighlight">\(i\)</span> is one value (a scalar).</p>
<p>But now we have <span class="math notranslate nohighlight">\(K\)</span> classes. The prediction is no longer one value but our hypothesis should output a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector. As a consequence, we will not have a vector for our model parameters but a collection of vectors <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(1)}, \boldsymbol{\theta}^{(2)}, \cdots,  \boldsymbol{\theta}^{(K)}\)</span>, each of <span class="math notranslate nohighlight">\(n+1\)</span> elements. It is commonly represented as a matrix <span class="math notranslate nohighlight">\(\Theta \in \mathbb{R}^{(n+1)\times K}\)</span>, where these vectors are placed in columns:</p>
<div class="math notranslate nohighlight" id="equation-matrixthetas">
<span class="eqno">(22)<a class="headerlink" href="#equation-matrixthetas" title="Permalink to this equation">#</a></span>\[\begin{split}\Theta \;=\;
\begin{bmatrix}
\,|  &amp; | &amp; &amp; | \\
\boldsymbol{\theta}^{(1)} &amp; \boldsymbol{\theta}^{(2)} &amp; \cdots &amp; \boldsymbol{\theta}^{(K)} \\
\,| &amp; | &amp; &amp; | 
\end{bmatrix}\end{split}\]</div>
<p>Now we need to modiy our logistic function (the sigmoid) to adapt to this.</p>
</section>
<section id="the-softmax">
<h3>The softmax<a class="headerlink" href="#the-softmax" title="Permalink to this headline">#</a></h3>
<p>Let‚Äôs define a generalized sigmoid, an extension of the logistic function for <span class="math notranslate nohighlight">\(K\)</span> classes. This is called the softmax function.</p>
<div class="proof definition admonition" id="softmaxdef">
<p class="admonition-title"><span class="caption-number">Definition 19 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>softmax</strong> function, also known as <strong>softargmax</strong>‚Ää or <strong>normalized exponential function</strong>, for a given class <span class="math notranslate nohighlight">\(k\)</span> among <span class="math notranslate nohighlight">\(K\)</span> classes, is defined as:</p>
<div class="math notranslate nohighlight" id="equation-softmaxeq">
<span class="eqno">(23)<a class="headerlink" href="#equation-softmaxeq" title="Permalink to this equation">#</a></span>\[\hat{y}_k^{(i)} = \frac{ \exp\!\big(z_k^{(i)}\big)}{\displaystyle \sum_{c=1}^K \exp\!\big(z_c^{(i)}\big)}, \quad k=1,\dots,K\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_k^{(i)}\)</span> is the output prediction for the data sample <span class="math notranslate nohighlight">\(i\)</span> for the class <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
</div><p>In a matrix form, it looks like this:</p>
<div class="math notranslate nohighlight" id="equation-softmaxmatrixeq">
<span class="eqno">(24)<a class="headerlink" href="#equation-softmaxmatrixeq" title="Permalink to this equation">#</a></span>\[\begin{split}h_\boldsymbol{\Theta}(\boldsymbol{x}) =
\begin{bmatrix}
P(y=1 \mid \boldsymbol{x}; \boldsymbol{\theta}^{(1)}) \\
P(y=2 \mid \boldsymbol{x}; \boldsymbol{\theta}^{(2)}) \\
\vdots \\
P(y=K \mid \boldsymbol{x}; \boldsymbol{\theta}^{(K)})
\end{bmatrix}
=
\frac{1}{\displaystyle \sum_{c=1}^{K} \exp\!\big(\boldsymbol{\theta}^{(c)\top} \boldsymbol{x}\big)}
\begin{bmatrix}
\exp\!\big(\boldsymbol{\theta}^{(1)\top} \boldsymbol{x}\big) \\
\exp\!\big(\boldsymbol{\theta}^{(2)\top} \boldsymbol{x}\big) \\
\vdots \\
\exp\!\big(\boldsymbol{\theta}^{(K)\top} \boldsymbol{x}\big)
\end{bmatrix}\end{split}\]</div>
<p>Note that the sum in the denominator normalizes the output.</p>
<p>So the pipeline in multiclass classification is, for a given sample <span class="math notranslate nohighlight">\(i\)</span> and class <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-pipelinemultiregeq">
<span class="eqno">(25)<a class="headerlink" href="#equation-pipelinemultiregeq" title="Permalink to this equation">#</a></span>\[\boldsymbol{x}^{(i)} \xrightarrow{ \text{linear map }\;\boldsymbol{\Theta} \boldsymbol{x}^{(i)}\;} z_k^{(i)}
\xrightarrow{\;\text{softmax }\;} \hat{y}_k^{(i)}\]</div>
<p>Now let‚Äôs use this to revisit our cross-entropy cost function!</p>
</section>
<section id="categorical-cross-entropy">
<h3>Categorical cross-entropy<a class="headerlink" href="#categorical-cross-entropy" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="catxentropydef">
<p class="admonition-title"><span class="caption-number">Definition 20 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>categorical cross-entropy</strong> cost function for multiclass logistic regression is defined as:</p>
<div class="math notranslate nohighlight" id="equation-catxentropyeq">
<span class="eqno">(26)<a class="headerlink" href="#equation-catxentropyeq" title="Permalink to this equation">#</a></span>\[C(\Theta) \;=\; -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K  y_k^{(i)} \, \log \hat{y}_k^{(i)}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m\)</span> is the number of training samples,</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> is the number of classes,</p></li>
<li><p><span class="math notranslate nohighlight">\(y_k^{(i)} \in \{0,1\}\)</span> is the indicator variable (1 if sample <span class="math notranslate nohighlight">\(i\)</span> belongs to class <span class="math notranslate nohighlight">\(k\)</span>, 0 otherwise),</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_k^{(i)} = P(y=k \mid \boldsymbol{x}^{(i)}; \Theta)\)</span> is the predicted probability for class <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ul>
</section>
</div><p>üí° Intuition: Just like in the binary case, each term is switched on or off by the class indicator <span class="math notranslate nohighlight">\(y^{(i)}_k\)</span>.</p>
<div class="seealso admonition">
<p class="admonition-title">Exercise</p>
<p>Check that with <span class="math notranslate nohighlight">\(K=2\)</span>, you get back to the binary cross-entropy.</p>
</div>
<p><strong>Brain teaser</strong><br />
The softmax has a sum over all classes at the denominator. How come the logistic function for the binary case does not?</p>
<p>If this bugs you, have a look at the reference below.</p>
<div class="seealso admonition">
<p class="admonition-title">Learn more</p>
<ul class="simple">
<li><p><a class="reference external" href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">Softmax Regression</a> from the <a class="reference external" href="http://ufldl.stanford.edu/tutorial/">UFLDL (Unsupervised Feature Learning and Deep Learning) Tutorial</a> (Stanford)</p></li>
</ul>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="log_reg_2_sigmoid.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">What is the Sigmoid Function?</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="log_reg_4_gradient_descent.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gradient Descent for Logistic Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    <div class="extra_footer">
      <div style="display:flex; align-items:center; gap:10px;">
  <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    <img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="CC Logo">
  </a>
  <div>
    By Claire David ‚Ä¢ ¬© 2025<br>
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
  </div>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>