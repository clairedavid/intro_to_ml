
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Bias &amp; Variance: a Tradeoff &#8212; Introduction&lt;br&gt; to Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Decision Trees" href="../trees/dt_0_and_boosting.html" />
    <link rel="prev" title="Let’s ROC!" href="model_eval_3_roc_curve.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction<br> to Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction to Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about/teaching_philosophy.html">
   Teaching philosophy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/instructor.html">
   Instructor &amp; Credits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/tools.html">
   Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The basics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linear_regression.html">
   Warm-up: Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_1_notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_2_cost_function.html">
     Cost Function in Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_3_gradient_descent_1d.html">
     Gradient Descent in 1D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_4_gradient_descent_multiD.html">
     Multivariate Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_5_learning_rate.html">
     Learning Rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lin_reg_6_gradient_descent_in_practice.html">
     Gradient Descent in Practice
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_1_intro.html">
     Logistic Regression: introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_2_sigmoid.html">
     What is the Sigmoid Function?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_3_cost_function.html">
     Cost Function for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_4_gradient_descent.html">
     Gradient Descent for Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="log_reg_5_multiclass.html">
     Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="model_evaluation.html">
   Model Evaluation
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_1_train_test_split.html">
     Splitting Datasets for Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_2_perf_metrics.html">
     Performance Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model_eval_3_roc_curve.html">
     Let’s ROC!
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Bias &amp; Variance: a Tradeoff
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Decision trees and boosting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/dt_0_and_boosting.html">
   Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_1_what_are_trees.html">
     What are Decision Trees?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_2_cart_algorithm.html">
     The CART Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/dt_3_limitations.html">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/el_0_intro.html">
   Ensemble Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_1_what_is_ensemble.html">
     What is Ensemble Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/el_2_random_forests.html">
     Random Forests
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../trees/boost_0_intro.html">
   Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_1_what_is_boosting.html">
     What is Boosting?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_2_adaboost.html">
     AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../trees/boost_3_gradient_boosting.html">
     Gradient Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../nn/nn_0_intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nn/nn_1_motivations.html">
   Motivations
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../nn/model_rep.html">
   Essential Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/model_rep_1_neurons.html">
     The basic units: neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/model_rep_2_activation_functions.html">
     Activation Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/model_rep_3_loss_cost.html">
     Loss and Cost Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../nn/training_0_intro.html">
   Training Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/training_1_forward_prop.html">
     Feedforward Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/training_2_backprop.html">
     Backpropagation Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/training_3_init.html">
     Initialization Schemes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../nn/optim_0_intro.html">
   Optimizing Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/optim_1_sdg.html">
     Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/optim_2_hyperparams.html">
     Hyperparameter Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/optim_3_lr_scheduling.html">
     Learning Rate Schedulers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nn/optim_4_adaptive_methods.html">
     Adaptive Optimizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nn/train_your_nn.html">
   Train Your Neural Net!
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_0_intro.html">
   Learning Without Labels?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_1_kmeans.html">
   Clustering: k-Means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_2_pca.html">
   Principal Component Analysis (PCA)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/ul_3_autoencoders.html">
   Autoencoders
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  STEP UP!
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../studies/studies_what_is_it.html">
   What is STEP?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../studies/studies_list.html">
     List of STEPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../studies/studies_submit_yours.html">
     Submit Your Own STEP!
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  AI Ethics &amp; Outlook
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ethics_outlook/ethics_0_intro.html">
   AI Ethics: What is it?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_1_definitions.html">
     Definitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_2_why_matters.html">
     Why Does It Matter?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ethics_outlook/ethics_3_resources.html">
     Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ethics_outlook/outlook.html">
   Outlook
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorial area
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t00_setup.html">
   Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t01_linear_regressor.html">
   T1. Linear Regressor
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t02_classifier.html">
   T2. Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t03_decision_stump.html">
   T3. Decision Stump
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t04_forestry.html">
   T4. Forestry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t05_nn_by_hand.html">
   T5. Neural Network by Hand!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/t06_unsupervised.html">
   T6. Unsupervised algorithms
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/basics/model_eval_4_bias_variance_tradeoff.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#underfitting-overfitting">
   Underfitting, overfitting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-to-cope-with-overtraining">
   Regularization to cope with overtraining
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularization">
     Lasso Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-definitions">
   Bias &amp; Variance: definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#illustratively">
   Illustratively
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-error">
   Generalization error
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition">
     Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-different-kinds-of-averages">
     Two different kinds of “averages”
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-derivation">
     Mathematical derivation
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Bias & Variance: a Tradeoff</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#underfitting-overfitting">
   Underfitting, overfitting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-to-cope-with-overtraining">
   Regularization to cope with overtraining
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularization">
     Lasso Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-definitions">
   Bias &amp; Variance: definitions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#illustratively">
   Illustratively
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalization-error">
   Generalization error
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decomposition">
     Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-different-kinds-of-averages">
     Two different kinds of “averages”
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-derivation">
     Mathematical derivation
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="bias-variance-a-tradeoff">
<h1>Bias &amp; Variance: a Tradeoff<a class="headerlink" href="#bias-variance-a-tradeoff" title="Permalink to this headline">#</a></h1>
<p>The bias and variance are the mathematical underpinnings of two distinct scenarios of mis-fit: underfitting and overfitting. Those situations are symptoms of high bias and high variance, respectively. Let’s first intuitively introduce under- and overfitting, then see how to diagnose them with convenient plots before presenting the bias-variance tradeoff. Last but not least, we will explore strategies to mitigate misfit and guide the model toward the optimal “sweet spot.”</p>
<section id="underfitting-overfitting">
<h2>Underfitting, overfitting<a class="headerlink" href="#underfitting-overfitting" title="Permalink to this headline">#</a></h2>
<p>Let’s have a look at the three cases below:</p>
<figure class="align-default" id="modeval-underoverfit">
<a class="reference internal image-reference" href="../_images/modEval_underoverfit.png"><img alt="../_images/modEval_underoverfit.png" src="../_images/modEval_underoverfit.png" style="width: 1200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">: Example of several regression models attempting to fit data points (blue dots) generated from a true function (orange curve). A fitting attempt is depicted in blue for a polynomial of degree 1 (left), degree 4 (middle) and degree 15 (right).</span><a class="headerlink" href="#modeval-underoverfit" title="Permalink to this image">#</a></p>
<div class="legend">
<p><sub>Source: <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html">scikit-learn.org (with associated python code)</a></sub></p>
</div>
</figcaption>
</figure>
<p>What can we say qualitatively about the quality of those fits? The linear one on the left (polynomial of degree 1) is clearly missing out the main pattern of the data. No matter the slope and the offset, a straight line will never fit a wavy curve decently. This is a case of underfitting. The model is too simple. On the other hand, the fit on the right (polynomial of degree 15) is perfectly passing through all datapoints. Technically, the performance is excellent! But the model is abusing its numerous degrees of freedom and has done more than fitting the data: it captured all the fluctuations and the noise specific to the given dataset. If we regenerate samples from the orange true function, the blue curve with large oscillations will definitely not pass through the newly generated points. There will be substantial errors. The excess of freedom with the high-degree polynomial is a bit of a curse here. This is a case of overfitting: the model is over-specific to the given random variations in the dataset. In the middle model, we seem to find a good compromise with a polynomial function of degree 4.</p>
<p>Hope this gives you a feel of the underfitting and overfitting (or undertraining and overtraining, synonyms). Now let’s write the definitions!</p>
<div class="proof definition admonition" id="underfittingdef">
<p class="admonition-title"><span class="caption-number">Definition 35 </span></p>
<section class="definition-content" id="proof-content">
<p><strong>Underfitting</strong> is a situation that occurs when a fitting procedure or machine learning algorithm is not capturing the general trend of the dataset.</p>
</section>
</div><p>Another way to put it: an underfit algorithm lacks complexity.</p>
<p>However, if we add too much complexity, we can fall in another trap: overfitting.</p>
<div class="proof definition admonition" id="overfittingdef">
<p class="admonition-title"><span class="caption-number">Definition 36 </span></p>
<section class="definition-content" id="proof-content">
<p><strong>Overfitting</strong> is a situation that occurs when a fitting procedure or machine learning algorithm matches too precisely a particular collection of a dataset.</p>
</section>
</div><p>Overfitting is synonym of overtuned, overtweaked. In other words: the model learns the details and noise in the training dataset to the extent that it negatively impacts the performance of the model on a new dataset. This means that the noise or random fluctuations in the training dataset are picked up and learned as actual trends by the model.
In machine learning, we look for general patterns and compromises: a good algorithm may not be perfectly classifying a given data set; it needs to accommodate and ignore rare outliers so that future predictions, on average, will be accurate.</p>
<p>The problem with overfitting is the future consequences once the machine learning algorithm receives additional data: it will very likely fail to generalize to new data.</p>
<div class="proof definition admonition" id="generalizationdef">
<p class="admonition-title"><span class="caption-number">Definition 37 </span></p>
<section class="definition-content" id="proof-content">
<p>In machine learning, <strong>generalization</strong> is the ability of a trained model to perform well on new, unseen data drawn from the same distribution as the training data.</p>
</section>
</div><div class="proof definition admonition" id="capacitydef">
<p class="admonition-title"><span class="caption-number">Definition 38 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>capacity</strong> of a model determines its ability to remember information about it’s training data.</p>
</section>
</div><p>Capacity is not a formal term, but it is linked to the number of degrees of freedom available to the model. A linear fit (two degrees of freedom) is limited to match a parabola. The model lacks capacity. However too much capacity can backfire! As in our example below, a model with many degrees of freedom will be so free that it is likely to fit some fluctuations. Problem.</p>
<p>But there is a trick! Let’s have those degrees of freedom (you get that with low-degree polynomials, we are stuck) but let’s add a constraint so as to tame the overfitting. This is called regularization.</p>
</section>
<section id="regularization-to-cope-with-overtraining">
<span id="modeval-reg"></span><h2>Regularization to cope with overtraining<a class="headerlink" href="#regularization-to-cope-with-overtraining" title="Permalink to this headline">#</a></h2>
<p>In <a class="reference internal" href="#modeval-underoverfit"><span class="std std-numref">Fig. 22</span></a>, the model on the right that overfits the data is extremely wiggly. To produce such sharp peaks, some of the polynomial’s coefficients (the model parameters) must take on very large values to “force” the curve through the points. What if we added the values of the model parameters (the <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> components) to the cost function? Since the cost is to be minimized, this could discourage the curve from becoming too wiggly. This is the idea behind the most common regularization techniques.</p>
<div class="proof definition admonition" id="regularizationdef">
<p class="admonition-title"><span class="caption-number">Definition 39 </span></p>
<section class="definition-content" id="proof-content">
<p><strong>Regularization</strong> in machine learning is a process consisting of adding constraints on a model’s parameters.</p>
</section>
</div><p>The two main types of regularization techniques are the Ridge Regularization (also known as <a class="reference external" href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a>, albeit the latter is more general) and the Lasso Regularization.</p>
<section id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">#</a></h3>
<p>The Ridge regression is a linear regression with an additional regularization term added to the cost function:</p>
<div class="math notranslate nohighlight" id="equation-ridgeeq">
<span class="eqno">(39)<a class="headerlink" href="#equation-ridgeeq" title="Permalink to this equation">#</a></span>\[C^{\text{reg}}(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \big( \hat{y}^{(i)} - y^{(i)} \big)^2
+ {\color{Maroon}\frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2}\]</div>
<p>The hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> controls the degree of regularization. If <span class="math notranslate nohighlight">\(\lambda = 0\)</span>, the regularization term vanishes and we have a non-regularized linear regression. You can see the penalty imposed by the term <span class="math notranslate nohighlight">\(\lambda\)</span> will force the parameters <span class="math notranslate nohighlight">\(\theta\)</span> to be as small as possible; this helps avoiding overfitting. If <span class="math notranslate nohighlight">\(\lambda\)</span> gets very large, the parameters can be so shrinked that the model becomes over-simplified to a straight line and thus underfit the data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The factor <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> is used in some derivations of the regularization. This makes it easier to calculate the gradient, however it is only a constant value that can be compensated by the choice of the hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The offset parameter <span class="math notranslate nohighlight">\(\theta_0\)</span> is not entering in the regularization sum.</p>
</div>
<p>In the litterature, the parameters are denoted with <span class="math notranslate nohighlight">\(b\)</span> for the offset (bias) and a vector of weight <span class="math notranslate nohighlight">\(\vec{w}\)</span> for the other parameters <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \cdots \theta_n\)</span>. Thus the regularization term is written:</p>
<div class="math notranslate nohighlight" id="equation-regl2weq">
<span class="eqno">(40)<a class="headerlink" href="#equation-regl2weq" title="Permalink to this equation">#</a></span>\[\lambda \left(\left\| \vec{w} \right\|_2\right)^2\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The <span class="math notranslate nohighlight">\(\ell_2\)</span> norm is the Euclidian norm <span class="math notranslate nohighlight">\(\|\boldsymbol{x}\|_2 = \sqrt{\sum_{i=0}^{n} x_i^2}\)</span>.</p>
</aside>
<p>with <span class="math notranslate nohighlight">\(\left\| \vec{w} \right\|_2\)</span> the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of the weight vector.</p>
<p>For logistic regression, the regularized cost function becomes:</p>
<div class="math notranslate nohighlight" id="equation-ridgelogeq">
<span class="eqno">(41)<a class="headerlink" href="#equation-ridgelogeq" title="Permalink to this equation">#</a></span>\[C^{\text{reg}}(\theta) = - \frac{1}{m} \sum_{i=1}^m \left[ \;\;
y^{(i)} \log\big( \hat{y}^{(i)} \big)
\;+\;
(1 - y^{(i)}) \log\big( 1 - \hat{y}^{(i)} \big)
\;\;\right]
+ {\color{Maroon}\frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2}\]</div>
<p>This is called L2-regularized logistic regression, or sometimes just ridge logistic regression.</p>
</section>
<section id="lasso-regularization">
<span id="class-algs-reg-lasso"></span><h3>Lasso Regularization<a class="headerlink" href="#lasso-regularization" title="Permalink to this headline">#</a></h3>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The <span class="math notranslate nohighlight">\(\ell_1\)</span> norm is the sum of the magnitudes of the vectors. It is also called Manhattan or <a class="reference external" href="https://en.wikipedia.org/wiki/Taxicab_geometry">Taxicab norm</a>.</p>
</aside>
<p>Lasso stands for least absolute shrinkage and selection operator. Behind the long acronym is a regularization of the linear regression using the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm. We denote Cost(<span class="math notranslate nohighlight">\(\theta\)</span>) the cost function, i.e. either the Mean Squared Error for linear regression or the cross-entropy loss function <a class="reference internal" href="log_reg_3_cost_function.html#equation-costfunctionlogreg">(19)</a> for logistic regression. The lasso regression cost function is</p>
<div class="math notranslate nohighlight" id="equation-lassocostf">
<span class="eqno">(42)<a class="headerlink" href="#equation-lassocostf" title="Permalink to this equation">#</a></span>\[C^{\text{reg}}(\theta) = \text{Cost(}{\theta}\text{)}  + {\color{Maroon}\frac{\lambda}{2m} \sum_{j=1}^n | \theta_j | }\]</div>
<p>The regularizing term uses the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm of the weight vector: <span class="math notranslate nohighlight">\(\left\| \vec{w} \right\|_1\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As regularization influences the parameters, it is important to first perform feature scaling before applying the regularization.</p>
</div>
<p><strong>Which regularization method to use?</strong></p>
<p>Each one has its pros and cons. As <span class="math notranslate nohighlight">\(\ell_1\)</span> (lasso) is a sum of absolute values, it is not differentiable and thus more computationally expensive. Yet <span class="math notranslate nohighlight">\(\ell_1\)</span> better deals with outliers (extreme values in the data) by not squaring their values. It is said to be more robust, i.e. more resilient to outliers in a dataset.</p>
<p>The <span class="math notranslate nohighlight">\(\ell_1\)</span> regularization, by shriking some parameters to zero (making them vanish and no more influencial), has <em>feature selection</em> built in by design. If this can be advantageous in some cases, it can mishandle highly correlated features by arbitrarily selecting one over the others.</p>
<p>Additional reading are provided below to deepen your understanding in the different regularization methods. Take home message: both methods combat overfitting.</p>
<div class="seealso admonition">
<p class="admonition-title">Learn more</p>
<ul class="simple">
<li><p>A comparison of the pros and cons of Ridge (<span class="math notranslate nohighlight">\(\ell_2\)</span> norm) and lasso (<span class="math notranslate nohighlight">\(\ell_1\)</span> norm) regularization: <a class="reference external" href="https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms/notebook">“L1 Norms versus L2 Norms”, Kaggle</a></p></li>
<li><p><a class="reference external" href="https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization">Fighting overfitting with <span class="math notranslate nohighlight">\(\ell_1\)</span> or <span class="math notranslate nohighlight">\(\ell_2\)</span> regularization, neptune.ai</a></p></li>
</ul>
</div>
</section>
</section>
<section id="bias-variance-definitions">
<h2>Bias &amp; Variance: definitions<a class="headerlink" href="#bias-variance-definitions" title="Permalink to this headline">#</a></h2>
<p>Underfitting and overfitting are symptoms of a fundamental modeling issue. This tension is captured by the notions of bias and variance. We will define them, see how they combine in the total error, and why it is not possible to reduce both simultaneously. This is the infamous dilemma of the bias–variance tradeoff.</p>
<section id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">#</a></h3>
<p>The bias systematic error coming from wrong assumptions on the model.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="important admonition">
<p class="admonition-title">Slight change of notations</p>
<p>Earlier in this course, we used <span class="math notranslate nohighlight">\(h_\theta(x)\)</span> as our model’s hypothesis function. For coherence in the notation, we will use <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}}(x)\)</span>, which refers to the trained/fitted model, i.e., <span class="math notranslate nohighlight">\(h_\theta(x)\)</span> after training, with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\theta}}\)</span> learned from the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. So <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}}\)</span> is basically the post-training version of <span class="math notranslate nohighlight">\(h_\theta\)</span>.</p>
</div>
</aside>
<div class="proof definition admonition" id="biasdef">
<p class="admonition-title"><span class="caption-number">Definition 40 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>bias</strong> measures how much, on average across different training sets <span class="math notranslate nohighlight">\(\mathcal{D} = \mathcal{D}_1, \cdots, \mathcal{D}_N\)</span>, the model’s predictions <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}_k}(x)\)</span> deviate from the true underlying function <span class="math notranslate nohighlight">\(f(x)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-biaseq">
<span class="eqno">(43)<a class="headerlink" href="#equation-biaseq" title="Permalink to this equation">#</a></span>\[\text{Bias}(x) = \mathbb{E}[\hat{f}_{\mathcal{D}}(x)] - f(x)\]</div>
</section>
</div><p>A highly biased model will most likely underfit the data.</p>
<div class="important admonition">
<p class="admonition-title">Important</p>
<p>The true distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span> from which the data are drawn is unknown, so the true function <span class="math notranslate nohighlight">\(f(x)\)</span> cannot be observed directly. We can write it down, but we will never have direct access to it.</p>
</div>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="variancedef">
<p class="admonition-title"><span class="caption-number">Definition 41 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>variance</strong> is a measure of how sensitive the model’s predictions are to random fluctuations in the training data.</p>
<div class="math notranslate nohighlight" id="equation-vareq">
<span class="eqno">(44)<a class="headerlink" href="#equation-vareq" title="Permalink to this equation">#</a></span>\[\text{Variance}(x) = \mathbb{E} \Bigl[ \bigl( \hat{f}_{\mathcal{D}}(x) - \mathbb{E}[\hat{f}_{\mathcal{D}}(x)] \bigr)^2 \Bigr]\]</div>
</section>
</div><p>As its name suggest, a model incorporating fluctuations in its design will change, aka <em>vary</em>, as soon as it is presented with new data (fluctuating differently). Variance is about the instability of the model due to training data variability.</p>
<p>A model with high variance is likely to overfit the data.</p>
<p>Using a larger training dataset will reduce the variance. However, extremely high-variance models may still fluctuate, but in general, more data helps.</p>
</section>
</section>
<section id="illustratively">
<h2>Illustratively<a class="headerlink" href="#illustratively" title="Permalink to this headline">#</a></h2>
<p>Below is a good visualization of the two tendencies for both regression and classification:</p>
<figure class="align-default" id="modeval-underoverfit-reg-class-table">
<a class="reference internal image-reference" href="../_images/modEval_underoverfit_reg_class_table.jpg"><img alt="../_images/modEval_underoverfit_reg_class_table.jpg" src="../_images/modEval_underoverfit_reg_class_table.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">: Illustration of situations of high bias (left) and high variance (right) for regression and classification.<br />
<sub>Image: LinkedIn Machine Learning India</sub></span><a class="headerlink" href="#modeval-underoverfit-reg-class-table" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can see that the left column deals with models too simple, very low in capacity, so completely failing to get the main data patterns. As a result, we will have a high cost (or error) with data used during the training as well as new data points to test the generalization. No matter how many new data points we add. This is a high bias situation – model too simple – and it can be spotted with the fact that <strong>errors will be large on both the training and the testing datasets</strong>.</p>
<p>Now, if the capacity is increased – with a higher degree polynomial or advanced model architecture – we can get a high variance situation. In that case, the error on the training dataset will be small. For a moment, we can think the model is great! However, if we test the model with new, unseen-during-training data, the fluctuations will differ and thus we will get a large testing error. This is a high variance situation. In that case, the errors will be low on the training dataset but large on the testing dataset.</p>
<p>To see which situation we are in, we compare errors on the training and test sets.</p>
<figure class="align-default" id="modeval-training-test-error">
<a class="reference internal image-reference" href="../_images/modEval_training_test_error.png"><img alt="../_images/modEval_training_test_error.png" src="../_images/modEval_training_test_error.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">Visualization of the error (cost function) with respect to the model’s complexity for the training and test sets. The ideal complexity is in the middle region where both the training and test errors are low and close to one another.<br />
<sub>Image: from the author, inspired by <a class="reference external" href="https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/">David Ziganto</a></sub></span><a class="headerlink" href="#modeval-training-test-error" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Two takeaways from this:</p>
<ol class="simple">
<li><p>The reduction of bias is done at the expend of an increase in variance. That’s unavoidable and we will soon see this both mathematically and practically</p></li>
<li><p>There is a golden mean, a zone where the model is compromising both on the bias and variance. That corresponds to the bottom of the test error. In this zone, the model captures the main pattern of the data and the generalization to new data is minimized.</p></li>
</ol>
</section>
<section id="generalization-error">
<h2>Generalization error<a class="headerlink" href="#generalization-error" title="Permalink to this headline">#</a></h2>
<div class="proof definition admonition" id="generrordef">
<p class="admonition-title"><span class="caption-number">Definition 42 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>generalization error</strong> of a model quantifies its expected loss on new data drawn from the same distribution as the training set.</p>
</section>
</div><p>In other words, the generalization error reflects how well the model captures essential patterns in the training data and transfers them to give accurate predictions on unseen data.</p>
<section id="decomposition">
<h3>Decomposition<a class="headerlink" href="#decomposition" title="Permalink to this headline">#</a></h3>
<p>The generalization error can be expressed as a sum of three errors:</p>
<div class="math notranslate nohighlight" id="equation-decompeqenglish">
<span class="eqno">(45)<a class="headerlink" href="#equation-decompeqenglish" title="Permalink to this equation">#</a></span>\[\text{Expected Test Error} =
\underbrace{\text{Bias}^2}_{\text{systematic error}}
+
\underbrace{\text{Variance}}_{\text{sensitivity to data}}
+
\underbrace{\sigma^2}_{\text{irreducible noise}}\]</div>
<p>This is the (infamous) bias-variance decomposition.</p>
<p>As the equation shows, the generalization error is an “expectation” of the loss on the test dataset. Put differently, it is a theoretical average over all possible unseen samples.
The first two terms in the sum are reducible with a smart choice of model complexity, as we saw before. The third term comes from the fact that data is noisy. Beyond careful data cleaning and smart preprocessing, there is little we can do: there will always be some noise. It’s irreducible.</p>
<p>Some of you may wonder (or have forgotten): why is the bias squared?</p>
<p>The best way to know is to derive yourself the bias-variance decomposition. But before, let’s note an important point on what is being averaged over here.</p>
</section>
<section id="two-different-kinds-of-averages">
<h3>Two different kinds of “averages”<a class="headerlink" href="#two-different-kinds-of-averages" title="Permalink to this headline">#</a></h3>
<p>An important point to note here in the bias–variance decomposition: the expectation is not over examples in the dataset! It’s over different possible training datasets.
Bias and variance are defined <em>pointwise</em>.</p>
<p>Let’s take one point <span class="math notranslate nohighlight">\(x^\text{test}\)</span> from our test data.</p>
<p>To get the bias and variance on that point <span class="math notranslate nohighlight">\(x^\text{test}\)</span>, we need to:</p>
<ul class="simple">
<li><p>Train our model on dataset <span class="math notranslate nohighlight">\(\mathcal{D}_1\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> we get predictor <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}_1}(x^\text{test})\)</span></p></li>
<li><p>Train our model on dataset <span class="math notranslate nohighlight">\(\mathcal{D}_2\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> we get predictor <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}_2}(x^\text{test})\)</span></p></li>
<li><p>…</p></li>
<li><p>Train our model on dataset <span class="math notranslate nohighlight">\(\mathcal{D}_N\)</span> <span class="math notranslate nohighlight">\(\rightarrow\)</span> we get predictor <span class="math notranslate nohighlight">\(\hat{f}_{\mathcal{D}_N}(x^\text{test})\)</span></p></li>
</ul>
<p>Now, we can define the following:</p>
<div class="proof definition admonition" id="expmodeldef">
<p class="admonition-title"><span class="caption-number">Definition 43 </span></p>
<section class="definition-content" id="proof-content">
<p>The <strong>expectation of a model</strong> at a point <span class="math notranslate nohighlight">\(x\)</span> is the average of the predictions <span class="math notranslate nohighlight">\(\hat{f}_\mathcal{D}(x)\)</span> over many training datasets <span class="math notranslate nohighlight">\(\mathcal{D} = \mathcal{D}_1, \cdots, \mathcal{D}_N\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-expmodeleq">
<span class="eqno">(46)<a class="headerlink" href="#equation-expmodeleq" title="Permalink to this equation">#</a></span>\[\mathbb{E}_\mathcal{D} [ \hat{f}_\mathcal{D}(x) ]\]</div>
</section>
</div></section>
<section id="mathematical-derivation">
<h3>Mathematical derivation<a class="headerlink" href="#mathematical-derivation" title="Permalink to this headline">#</a></h3>
<p>There won’t be the answer here, but guidance is available on demand. Try it yourself first! (Guaranteed joy after solving it with your own brain.)</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Hint on the noise</p>
<p>The true function <span class="math notranslate nohighlight">\(f(x)\)</span> defines the relationship between <span class="math notranslate nohighlight">\(X\)</span> and the targets <span class="math notranslate nohighlight">\(y\)</span>. But there will be some deviation due to noise. We can model this with an error term <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-yxerroreq">
<span class="eqno">(47)<a class="headerlink" href="#equation-yxerroreq" title="Permalink to this equation">#</a></span>\[y = f(x) + \epsilon\]</div>
<p>We assume it is normally distributed and with a standard deviation of <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Hint on how to start</p>
<p>Start by writing the expression of an expectation of the error:</p>
<div class="math notranslate nohighlight" id="equation-errorstarteq">
<span class="eqno">(48)<a class="headerlink" href="#equation-errorstarteq" title="Permalink to this equation">#</a></span>\[\text{Expected Squared Test Error} = \text{Test MSE} = \mathbb{E} \Big[ \big(y^\text{test} - \hat{f}_\mathcal{D}(x^\text{test})\big)^2 \Big]\]</div>
<p>For the derivation, you can omit the ‘test’ upperscript to lighten the equations a bit.</p>
<p>Now? Expand this expression and use the properties you know about expectations and variances…</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Hint on a useful relation</p>
<p>Recall that the variance of a random variable <span class="math notranslate nohighlight">\(X\)</span> can be written in two ways:</p>
<div class="math notranslate nohighlight" id="equation-vareqs">
<span class="eqno">(49)<a class="headerlink" href="#equation-vareqs" title="Permalink to this equation">#</a></span>\[\mathrm{Var}(X) = \mathbb{E} \big[ (X - \mathbb{E}[X])^2 \big] = \mathbb{E}[X^2] - \mathbb{E}[X]^2\]</div>
<p>You will need this!</p>
</div>
<p> <br />
 <br />
 <br />
 <br />
 <br />
 </p>
<div class="seealso admonition">
<p class="admonition-title">Check it!    (after you tried hard)</p>
<p>Below are two derivations:</p>
<ul class="simple">
<li><p>The very accessible and easy-to-follow format by Allen Akinkunle: <a class="reference external" href="https://allenkunle.me/bias-variance-decomposition">The Bias-Variance Decomposition Demystified</a>. The illustrations with simulated data are excellent to really grasp the tradeoff. I suggest opening a notebook and giving it a try!</p></li>
<li><p>The thorough <a class="reference external" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html">Lecture 12: Bias-Variance Tradeoff</a> by Kilian Weinberger, from the course Machine Learning for Intelligent Systems taught at Cornell University.</p></li>
</ul>
</div>
<p><strong>Brain teaser</strong><br />
Would the decomposition Bias<span class="math notranslate nohighlight">\(^2\)</span> + Variance still hold for a loss that is <strong>not</strong> the MSE? 🤔</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="model_eval_3_roc_curve.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Let’s ROC!</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../trees/dt_0_and_boosting.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decision Trees</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    <div class="extra_footer">
      <div style="display:flex; align-items:center; gap:10px;">
  <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
    <img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="CC Logo">
  </a>
  <div>
    By Claire David • © 2025<br>
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
  </div>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>